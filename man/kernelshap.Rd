% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kernelshap.R
\name{kernelshap}
\alias{kernelshap}
\alias{kernelshap.default}
\alias{kernelshap.ranger}
\alias{kernelshap.Learner}
\title{Kernel SHAP}
\usage{
kernelshap(object, ...)

\method{kernelshap}{default}(
  object,
  X,
  bg_X,
  pred_fun = stats::predict,
  bg_w = NULL,
  sampling_strategy = c("auto", "paired", "exact", "simple"),
  paired_sampling = NULL,
  exact = NULL,
  m = NULL,
  tol = 0.01,
  max_iter = 250,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

\method{kernelshap}{ranger}(
  object,
  X,
  bg_X,
  pred_fun = function(m, X, ...) stats::predict(m, X, ...)$predictions,
  bg_w = NULL,
  sampling_strategy = c("auto", "paired", "exact", "simple"),
  paired_sampling = NULL,
  exact = NULL,
  m = NULL,
  tol = 0.01,
  max_iter = 250,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)

\method{kernelshap}{Learner}(
  object,
  X,
  bg_X,
  pred_fun = function(m, X) m$predict_newdata(X)$response,
  bg_w = NULL,
  sampling_strategy = c("auto", "paired", "exact", "simple"),
  paired_sampling = NULL,
  exact = NULL,
  m = NULL,
  tol = 0.01,
  max_iter = 250,
  parallel = FALSE,
  parallel_args = NULL,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{object}{Fitted model object.}

\item{...}{Additional arguments passed to \code{pred_fun(object, X, ...)}.}

\item{X}{A (n x p) matrix, data.frame, tibble or data.table of rows to be explained.
Important: The columns should only represent model features, not the response.}

\item{bg_X}{Background data used to integrate out "switched off" features,
often a subset of the training data (around 100 to 200 rows)
It should contain the same columns as \code{X}. Columns not in \code{X} are silently
dropped and the columns are arranged into the order as they appear in \code{X}.}

\item{pred_fun}{Prediction function of the form \code{function(object, X, ...)},
providing K >= 1 numeric predictions per row. Its first argument represents the
model \code{object}, its second argument a data structure like \code{X}.
(The names of the first two arguments do not matter.) Additional (named)
arguments are passed via \code{...}. The default, \code{stats::predict}, will
work in most cases. Some exceptions (classes "ranger" and mlr3 "Learner")
are handled separately. In other cases, the function must be specified manually.}

\item{bg_w}{Optional vector of case weights for each row of \code{bg_X}.}

\item{sampling_strategy}{Sampling strategy to draw m binary "on-off" vectors z of
length p used to calculate Kernel SHAP. Currently, one of:
\itemize{
\item \code{"auto"} (default): "exact" for up to 8 features, and "paired" otherwise.
\item \code{"simple"}: Each z is drawn randomly according to Kernel SHAP weights.
This is done in two steps: First, the number s of "1" is drawn from the Kernel weight
distribution (normalized to the range from 1 to p-1), see Covert and Lee (2021).
Then, the s random positions of a length p 0-vector are set to 1.
Note that this strategy is strictly worse than paired sampling, so there is
virtually no reason to use it except for studying properties of Kernel SHAP.
\item \code{"paired"}: Here, m/2 vectors z are drawn as with the "simple" strategy.
Then, all m/2 complements 1-z are added to the pool of vectors.
This paired sampling strategy converges faster than the simple method,
see Covert and Lee (2021).
\item \code{"exact"}: All possible binary vectors z are enumerated and weighted
according to the SHAP kernel weight distribution. This produces exact Kernel SHAP
values with respect to the given background data. The algorithm works with large
prediction data having (2^p-2) * nrow(bg_X) rows. Thus, we recommend this option
up to p=8.
}}

\item{paired_sampling}{Deprecated, set \code{sampling_strategy = "paired"}.}

\item{exact}{Deprecated, set \code{sampling_strategy = "exact"}.}

\item{m}{Number of on-off vectors to be evaluated during one iteration.
The default, \code{NULL}, equals \code{2*max(trunc(20*sqrt(p)), 5*p)}, where p is the
number of features. Ignored if exact calculations are done.}

\item{tol}{Tolerance determining when to stop. The algorithm keeps iterating until
max(sigma_n) / diff(range(beta_n)) < tol, where the beta_n are the SHAP values
of a given observation and sigma_n their standard errors. For multidimensional
predictions, the criterion must be satisfied for each dimension separately.
The stopping criterion uses the fact that standard errors and SHAP values are all
on the same scale. Ignored if exact calculations are done.}

\item{max_iter}{If the stopping criterion (see \code{tol}) is not reached after
\code{max_iter} iterations, the algorithm stops. Ignored if exact calculations are done.}

\item{parallel}{If \code{TRUE}, use parallel \code{foreach::foreach()} to loop over rows
to be explained. Must register backend beforehand, e.g. via "doFuture" package,
see Readme for an example. Parallelization automatically disables the progress bar.}

\item{parallel_args}{A named list of arguments passed to \code{foreach::foreach()}, see
\code{?foreach::foreach}. Ideally, this is \code{NULL} (default). Only relevant
if \code{parallel = TRUE}. Example on Windows: if \code{object} is a generalized
additive model fitted with package "mgcv", then one might need to set
\code{parallel_args = list(.packages = "mgcv")}.}

\item{verbose}{Set to \code{FALSE} to suppress messages and the progress bar.}
}
\value{
An object of class "kernelshap" with the following components:
\itemize{
\item \code{S}: (n x p) matrix with SHAP values or, if the model output has dimension K > 1,
a list of K such matrices.
\item \code{X}: Same as input argument \code{X}.
\item \code{baseline}: A vector of length K representing the average prediction on the background data.
\item \code{SE}: Standard errors corresponding to \code{S} (and organized like \code{S}).
\item \code{n_iter}: Integer vector of length n providing the number of iterations per row of \code{X}.
\item \code{converged}: Logical vector of length n indicating convergence per row of \code{X}.
\item \code{m}: Integer providing the effective number of on-off vectors used per iteration.
}
}
\description{
Implements a multidimensional version of the Kernel SHAP algorithm explained in
detail in Covert and Lee (2021). It is an iterative refinement of the original
Kernel SHAP algorithm of Lundberg and Lee (2017). The algorithm is applied to each
row in \code{X}. Its behaviour depends on the number of features p:
\itemize{
\item 2 <= p <= 8: Exact Kernel SHAP values are returned.
(Exact regarding the given background data.)
\item p > 8: Sampling version of Kernel SHAP.
The algorithm iterates until Kernel SHAP values are sufficiently accurate.
Approximate standard errors of the SHAP values are returned.
\item p = 1: Exact Shapley values are returned.
}
}
\details{
During each iteration, \code{m} on-off vectors (feature subsets) are evaluated until
the worst standard error of the SHAP values is small enough relative to the range of
the SHAP values. This stopping criterion was suggested in Covert and Lee (2021) and
uses the fact that SHAP values and their standard errors are all on the scale of the
predictions. In the multi-output case, the criterion must be fulfilled for each
dimension separately until iteration stops.
}
\section{Methods (by class)}{
\itemize{
\item \code{kernelshap(default)}: Default Kernel SHAP method.

\item \code{kernelshap(ranger)}: Kernel SHAP method for "ranger" models, see Readme for an example.

\item \code{kernelshap(Learner)}: Kernel SHAP method for "mlr3" models, see Readme for an example.

}}
\examples{
# Linear regression
fit <- stats::lm(Sepal.Length ~ ., data = iris)
s <- kernelshap(fit, iris[1:2, -1], bg_X = iris)
s

# Multivariate model
fit <- stats::lm(
  as.matrix(iris[1:2]) ~ Petal.Length + Petal.Width + Species, data = iris
)
s <- kernelshap(fit, iris[1:4, 3:5], bg_X = iris)
s

# Matrix input works as well, and pred_fun can be overwritten
fit <- stats::lm(Sepal.Length ~ ., data = iris[1:4])
pred_fun <- function(fit, X) stats::predict(fit, as.data.frame(X))
X <- data.matrix(iris[2:4])
s <- kernelshap(fit, X[1:3, ], bg_X = X, pred_fun = pred_fun)
s

# Logistic regression
fit <- stats::glm(
  I(Species == "virginica") ~ Sepal.Length + Sepal.Width, 
  data = iris, 
  family = binomial
)

# On scale of linear predictor
s <- kernelshap(fit, iris[1:2], bg_X = iris)
s

# On scale of response (probability)
s <- kernelshap(fit, iris[1:2], bg_X = iris, type = "response")
s

}
\references{
\enumerate{
\item Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression. Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:3457-3465, 2021.
\item Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems 30, 2017.
}
}
