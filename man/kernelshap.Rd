% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kernelshap.R
\name{kernelshap}
\alias{kernelshap}
\title{Kernel SHAP}
\usage{
kernelshap(
  X,
  pred_fun,
  bg_X,
  bg_w = NULL,
  paired_sampling = TRUE,
  m = "auto",
  exact = TRUE,
  tol = 0.01,
  max_iter = 250,
  parallel = FALSE,
  verbose = TRUE,
  ...
)
}
\arguments{
\item{X}{A (n x p) matrix, data.frame, tibble or data.table of rows to be explained.
Important: The columns should only represent model features, not the response.}

\item{pred_fun}{A function that takes a data structure like \code{X}
and provides K >= 1 numeric predictions per row.
Example: If "fit" denotes a logistic regression fitted via \code{stats::glm},
and SHAP values should be on the probability scale, then this argument is
\code{function(X) predict(fit, X, type = "response")}.}

\item{bg_X}{The background data used to integrate out "switched off" features.
It should contain the same columns as \code{X}. A good size is around 50 to 200 rows.
Columns not in \code{X} are silently dropped and the columns are arranged into
the order as they appear in \code{X}.}

\item{bg_w}{Optional vector of case weights for each row of \code{bg_X}.}

\item{paired_sampling}{Logical flag indicating whether to use paired sampling.
The default is \code{TRUE}. This means that with every feature subset S,
also its complement is evaluated, which leads to considerably faster convergence.}

\item{m}{Number of feature subsets S to be evaluated during one iteration.
The default, "auto", equals \code{max(trunc(20*sqrt(p)), 5*p)}, where p is the
number of features.
For the paired sampling strategy, 2m evaluations are done per iteration.}

\item{exact}{If \code{TRUE} (default) and the number of features p is at most 5,
the algorithm will produce exact Kernel SHAP values. In this case, the arguments
\code{m}, \code{paired_sampling}, \code{tol}, and \code{max_iter} are ignored.}

\item{tol}{Tolerance determining when to stop. The algorithm keeps iterating until
max(sigma_n) / diff(range(beta_n)) < tol, where the beta_n are the SHAP values
of a given observation and sigma_n their standard errors. For multidimensional
predictions, the criterion must be satisfied for each dimension separately.}

\item{max_iter}{If the stopping criterion (see \code{tol}) is not reached after
\code{max_iter} iterations, then the algorithm stops.}

\item{parallel}{If \code{TRUE}, use parallel \code{foreach} to loop over rows
to be explained. Must register backend beforehand, e.g. via \code{doMC}. See
example below. Parallelization automatically disables the progress bar.}

\item{verbose}{Set to \code{FALSE} to suppress messages, warnings, and the progress bar.}

\item{...}{Currently unused.}
}
\value{
An object of class "kernelshap" with the following components:
\itemize{
\item \code{S}: (n x p) matrix with SHAP values or, if the model output has dimension K > 1,
a list of K such matrices.
\item \code{X}: Same as input argument \code{X}.
\item \code{baseline}: A vector of length K representing the average prediction on the background data.
\item \code{SE}: Standard errors corresponding to \code{S} (and organized like \code{S}).
\item \code{n_iter}: Integer vector of length n providing the number of iterations per row of \code{X}.
\item \code{converged}: Logical vector of length n indicating convergence per row of \code{X}.
}
}
\description{
Implements a multidimensional version of the Kernel SHAP algorithm explained in detail in
Covert and Lee (2021). It is an iterative refinement of the original Kernel SHAP algorithm
of Lundberg and Lee (2017).
The algorithm is applied to each row in \code{X}. Its behaviour depends on the number of features p:
\itemize{
\item 2 <= p <= 5: Exact Kernel SHAP values are returned. (Exact regarding the given background data.)
\item p > 5: Sampling version of Kernel SHAP. The algorithm iterates until Kernel SHAP values are sufficiently accurate.
Approximate standard errors of the SHAP values are returned.
\item p = 1: Exact Shapley values are returned.
}
}
\details{
\code{X} should only contain feature columns required by the
prediction function \code{pred_fun}. The latter is a function taking
a data structure like \code{X} or \code{bg_X} and providing K >= 1 numeric
predictions per row. The background data \code{bg_X} must contain the same column names
as \code{X} (additional columns are silently dropped).
During each iteration, \code{m} feature subsets are evaluated until the worst
standard error of the SHAP values is small enough relative to the range of the SHAP values.
This stopping criterion was suggested in Covert and Lee (2021). In the multioutput case,
the criterion must be fulfilled for each dimension separately until iteration stops.
}
\examples{
fit <- stats::lm(Sepal.Length ~ ., data = iris)
pred_fun <- function(X) stats::predict(fit, X)
s <- kernelshap(iris[1:2, -1], pred_fun = pred_fun, iris[, -1])
s

# In parallel
require(doMC)
registerDoMC(cores = 4)
system.time(kernelshap(iris[, -1], pred_fun = pred_fun, iris[, -1]))
system.time(kernelshap(iris[, -1], pred_fun = pred_fun, iris[, -1], parallel = TRUE))

# Multioutput regression (or probabilistic classification)
fit <- stats::lm(
  as.matrix(iris[1:2]) ~ Petal.Length + Petal.Width + Species, data = iris
)
fit
s <- kernelshap(iris[1:4, 3:5], pred_fun = pred_fun, iris[, 3:5])
s

# Matrix input works as well, and pred_fun may contain preprocessing steps
fit <- stats::lm(Sepal.Length ~ ., data = iris[1:4])
pred_fun <- function(X) stats::predict(fit, as.data.frame(X))
X <- data.matrix(iris[2:4])
s <- kernelshap(X[1:3, ], pred_fun = pred_fun, X)
s
}
\references{
\enumerate{
\item Ian Covert and Su-In Lee. Improving KernelSHAP: Practical Shapley Value Estimation Using Linear Regression. Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, PMLR 130:3457-3465, 2021.
\item Scott M. Lundberg and Su-In Lee. A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems 30, 2017.
}
}
